package LeetCode.BigData;

/**
 * 布隆过滤器
 * 不安全网页黑名单包含100亿个黑名单网页，每个网页URL最多占据64B，实现一种网页
 * 过滤系统，更具网页的URL判断该网页是否在黑名单上，请设计该系统
 * 要求：系统允许有万分之一以下的判断失误率
 *      使用额外的空间复杂度不要超过30GB
 *
 *思路一：使用哈希表保持所有URL，至少需要640GB，不满足要求
 *思路二：使用布隆过滤器，允许一定程度的失误
 *      使用k个哈希函数将输入映射到一个输出域S，S为bit类型的数组，将每个
 *      输入做映射，对应的S为1；新的输入同样通过哈希函数映射，只要有一个哈希函数
 *      对应的S不为1，则不再黑名单
 *
 */

/**
 * 只用2Gb内存在20亿个整数中找到出现次数最多的数
 * 思路一：使用哈希表对每一个出现的数做词频统计，key需要4B，value需要4B
 * 一条记录需要8B，当哈希表记录数为2亿个时，需要至少1.6GB ,内存无法满足要求
 * 思路二：使用哈希函数将大数据分成16个小文件
 */


/**
 * 40亿个非负整数中找到没出现的数
 * 32位无符号整数的范围是0~4294967295，现在有一个正好包含40亿个无符号整数的
 * 文件，所以在整个范围中必然有没出现过的数，可以使用最多1GB的内存，怎么找到所有的数
 *
 * 思路一：使用哈希表保存出现过得数，最坏情况40亿个数都不一样，需要160亿字节
 * 大约需要16GB，不符合要求
 * 思路二：使用bit map的放肆表示数出现过的情况
 * 思路三：如果只需找到一个没出现的数，最多使用10MB
 *         可以将0~4294967295分为64个区间，遍历40亿个数，判断每个区间的数量
 *         至少存在一个区间的存在未出现过的数，在使用bitmap
 */


/**
 * 有一个包含100亿个URL的大文件，假设每个URL占用64B，请找出其中所有重复的URL
 * 思路：使用哈希函数分配到若干机器或者若干小文件，直到满足资源限制的条件
 */
public class BigData {
}
